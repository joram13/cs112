---
title: "Assignment 2, Spring 2021"
author: "Joram Erbarth"
date: "01/30/2021"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# Don't change the line below
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, 
                      message=FALSE, fig.width=6, fig.align="center")
# If you are using other packages, load them here. 
# If you don't have the following packages installed,
# please install them first. But don't include the installation
# code here because every time you knit this document they'll 
# be reinstalled which is not necessary!
library(Matching)
library(knitr)
library(janitor)
library(tidyverse)
library(arm)
library(caret)
# we need to set the seed of R's random number generator, 
# in order to produce comparable results 
set.seed(928)
```

# A few important notes

**Option 1 for submitting your assignment**: *This method is actually preferred. This is an RMarkdown document. Did you know you can open this document in RStudio, edit it by adding your answers and code, and then knit it to a pdf? To submit your answers to this assignment, simply knit this file as a pdf and submit it as a pdf on Forum. All of your code must be included in the resulting pdf file, i.e., don't set echo = FALSE in any of your code chunks. [This](https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf) is a cheat sheet for using Rmarkdown. If you have questions about RMarkdown, please post them on Piazza. Try knitting this document in your RStudio. You should be able to get a pdf file. At any step, you can try knitting the document and recreate a pdf. If you get an error, you might have incomplete code.*

**Option 2 for submitting your assignment**: *If you are not comfortable with RMarkdown, you can also choose the Google Doc version of this assignment, make a copy of it and edit the Google doc (include your code, figures, results, and explanations) and at the end download your Google Doc as a pdf and submit the pdf file.*

**Note**: *Either way (if you use Rmd and knit as pdf OR if you use Google Doc and download as pdf) you should make sure you put your name on top of the document.*

**Note**: *The first time you run this document you may get an error that some packages don't exist. If you don't have the packages listed on top of this document, install them first and you won't get those errors.*

**Note**: *Don't change seed in the document. The function `set.seed()` has already been set at the beginning of this document to 928. Changing the see again to a different number will make your results not replicable.*


## QUESTION 1: Data Generating Example

#### STEP 1

Create a set of 1000 outcome observations using a data-generating process (DGP) that incorporates two variables and a stochastic component (of your choice). In other words, create two independent variables, a vector of noise, and a dependent variable (outcome) that relates to the independent variables and the noise with a formula you choose.

```{r}
# Your code here

#create first independent variable with uniform distribution
team_position <- round(runif(1000, min = 0, max = 20))
#create second independent variable with uniform distribution
goals <- round(runif(1000, min = 0, max = 40))
#create noise with normal distribution
noise <- round(rnorm(1000, mean=0, sd = 1))
#create dependent variable with formula
value <- round(0.7*goals+0.3*team_position + noise)
```

For the DGP, I create two independent variables, the dependent variable, and noise. For the first independent variable, I randomly sample thousand times from a uniform distribution from 0 to 20, and round the results. For the second independent variable, I randomly sample thousand times from a uniform distribution from 0 to 40 and and round the results. For the noise I randomly sample thousand times from a normal distribution with mean 0 and standard deviation 1. Then I create the dependnet variable by applying a formula that infolves all other variables. 
#### STEP 2

Tell a 2-3 sentence story about the data generating process you coded up above. What is it about and what each component means?

The dependent variable value represents the market value of a football player. The independent variable goals represents the number of goals the player has scored in the last season, and the independent variable team_position represents the position his team is in in. The noise variable is the vector of noise.We assume that the market value of a player is equal to 0.7 times his goals plus 0.3 times the position of his team plus noise (in million euro). 

#### STEP 3

Fit a regression model of the outcome on the two independent variables and see if the coefficients you find are similar to the ones in your DGP. 

```{r}
# Your code here

# creating linear regression model
predict_value <- lm(value~goals+team_position)
#print values predicted using the linear regression model
predict_value
```

First, we create a model using linear regression (lm function). Then we print the coefficients of this model. We can see, that the coefficients we get are very close to the coefficients we used in the DGP. If we round the coefficients to one decimal place, they are equal. This shows that our linear regression produces a useful model for this data. 

#### STEP 4

Use the simulation-based approach covered in class (the arm library, etc.) to find the computational 95% confidence interval of your coefficients and report them here. Set the number of simulations to 10,000.

```{r}
# Your code here

#simulate 10000 coefficients for the predicted_vlaue model
sim_value <- sim(predict_value, n.sim = 10000)
#create 95% confidence intervals for all coefficients
quantile(sim_value@coef[,1], probs = c(0.025,0.975))
quantile(sim_value@coef[,2], probs = c(0.025,0.975))
quantile(sim_value@coef[,3], probs = c(0.025,0.975))
```

We can see that the 95% confidence interval for the intercept is (-0.1134774, 0.2282506), the interval for the goals coefficient is (0.6928976, 0.7046167), and the interval for the team_position coefficient is(0.2912706, 0.3138465). This shows that we are fairly certain about our coefficients. 

#### STEP 5

Now, estimate the 95% confidence interval for the predicted outcome when your first variable is equal to 1 and the second variable is equal to -2 using the simulated coefficients you found in Step 4. 

```{r}
# Your code here

# create new predictions using the linear regression and goals =1, team_position = -2
new_value_predict <- sim_value@coef[,1]+sim_value@coef[,2]*1+sim_value@coef[,3]*(-2)+sim_value@sigma
#calculate 95% confidence interval for the prediction
quantile(new_value_predict, probs = c(0.025,0.975))

```

We can see that the 95% confidence for the market value of a player with one goal and a position of his team of -2 is (1.046166, 1.424287). This is an extrapolation of the data, because the position variable can only be between 1 and 20.

## QUESTION 2: Outliers

Imagine that you want to create a data visualization that illustrates the sensitivity of regression to outlier data points. So, you want to create two figures:

One figure that shows a regression line fit to a 2-dimensional (x and y) scatterplot, such that the regression line clearly has a negative slope.

```{r}
# Your code here!

#create exercise data with a uniform distribution
exercise <- round(runif(30, min = 0, max = 14))
#create noise with a normal distribution
noise_2 <- round(rnorm(30, mean=0, sd = 20))
#create dependent variable weight with formula
weight <- 100 - 1.5* exercise + noise_2
#create linear model predicting weight based on exercise
pred_weight <- lm(weight~exercise)
#print model
print(pred_weight)
#plot data
plot(exercise, weight, main = "Weight of people against how often \n they excercise with regression line", xlab = "number of excercises per week", ylab = "weight in kg")
#plot regression line of model
abline(pred_weight)
```

And, another figure that shows a regression line with a positive slope fit to a scatter plot of the same data plus one additional outlier data point. This one data point is what changes the sign of the regression line’s slope from negative to positive.

```{r}
# Your code here!

#append an outlier: a persion with high weight and high numbers of exercise
weight_wo <- append(weight, 150)
exercise_wo <- append(exercise, 30)
#create a new linear regression model with outlier
pred_weight_wo <- lm(weight_wo~exercise_wo)
#print model
print(pred_weight_wo)
#plot data
plot(exercise_wo, weight_wo, main = "Weight of people against how often \n they excercise with regression line", xlab = "number of excercises per week", ylab = "weight in kg")
#plot regression line
abline(pred_weight_wo)

```

Be sure to label the axes and the title the figures appropriately. Include a brief paragraph that explains the number of observations and how you created the data set and the outlier.

First, we create the independent variable exercise. This variable states how often a week a person exercises. We sample it from a uniform distribution form 0 to 14. Further, we create the noise by sampling from a normal distribution with mean 0 and standard deviation 20. We use a formula to create weight data dependent on noise and exercise. We create a linear model using exercise to predict weight. We plot the data and the regression line, and can see a weak negative correlation: the more people exercise per week, the less they weight.
We add an outlier to the data: a person that weights a lot but exercises a lot too. We build a new linear regression model and can see that now the slope is positive. 
We have thse extreme changes because with 30 data points, we have a small sample size. Further, the outlier is far away from any other data points.  

## QUESTION 3: Simulation methods

You were hired by the local animal shelter at Austin, Texas to perform some data analysis. They are particularly interested in predicting how long an animal stays in a shelter given the information they have upon shelter admission. To focus on your task, you’ll be only looking at dogs.

They identified the following variables as relevant to the prediction:

- `Intake Type`: dogs can come into the shelter in multiple ways. They are interested in differentiating between three main ones: *strays* (most common: animals found without an owner by animal control), *owner surrenders* (animals brought to the shelter by their owners for various reasons), and *public assistance* (animals surrender by a person who is not their owner). The other type of intake is euthanasa request (made by the owner), which you should ignore. 

- `Intake Condition`: there are three conditions the shelter wants to mark — dogs identified as *Injured*, *Sick*, and *Nursing/Pregnant*. There are other conditions inserted in this field, but for dogs, they are not common. 

- `Age` (use `age_upon_intake_years`): how old was the dog, in years, when arriving at the shelter. 

- `Breed`: this is a fairly unreliable field, since people are not always successful at identifying the actual genetic breed of a dog. Nevertheless, you should try to look at a crude division: those identified as purebred compared to mixed breeds.

The variable containing the length of day, which you want to predict, is `time_in_shelter_days`. 

#### STEP 1

Download the dataset from [here](https://www.kaggle.com/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes?select=aac_intakes_outcomes.csv). Make sure you use `aac_intakes_outcomes.csv` which contains the joint intake and outcome data for all animals.

Then, perform the necessary preprocessing to be able to fit the model as required:

1. Remove `NA`s or empty entries in the fields you will need to use. 
2. Remove all entries except for dogs.
3. Use the `janitor` package and the `clean_names` function to convert all column names to all lowercase and replace spaces with underlines. The package is already loaded above but you should install it first if you don't have it.
4. Remove rows with the *Euthanasia Request* intake type. Make sure your `intake_type` column used for the model is a factor, so R will know to fit different coefficients for each condition (or alternatively, create dummy variables for each of the three yourself).
5. Create a new `intake_condition` feature that satisfies the shelter’s interests: dogs marked *normal*, *injured* and *sick* should have these values, but *nursing* and *pregnant* should be unified into a joint value (call it what you want) and all other values should be grouped under *other*. 

```{r}
# Your code here
#download data
aac_intakes_outcomes <- read_csv("aac_intakes_outcomes.csv")

#delete rows where intake_type is empty or NA (1)
aac_intakes_outcomes <- aac_intakes_outcomes[!(is.na(aac_intakes_outcomes$intake_type) | aac_intakes_outcomes$intake_type==""), ]
#delete rows where `age_upon_intake_(years)` is empty or NA (1)
aac_intakes_outcomes <- aac_intakes_outcomes[!(is.na(aac_intakes_outcomes$`age_upon_intake_(years)`) | aac_intakes_outcomes$`age_upon_intake_(years)`==""), ]
#delete rows where intake_condition is empty or NA (1)
aac_intakes_outcomes <- aac_intakes_outcomes[!(is.na(aac_intakes_outcomes$intake_condition) | aac_intakes_outcomes$intake_condition==""), ]
#delete rows where `time_in_shelter_days` is empty or NA (1)
aac_intakes_outcomes <- aac_intakes_outcomes[!(is.na(aac_intakes_outcomes$`time_in_shelter_days`) | aac_intakes_outcomes$`time_in_shelter_days`==""), ]

#consider only rows that have animal_type = dog (2)
aac_intakes_outcomes <- aac_intakes_outcomes[aac_intakes_outcomes$animal_type=="Dog", ]

#clean names (3)
aac_intakes_outcomes <- clean_names(aac_intakes_outcomes)

#remove rows with intake_type = euthanasia request (4)
aac_intakes_outcomes <- aac_intakes_outcomes[!( aac_intakes_outcomes$intake_type=="Euthanasia Request"), ]

#create new intake condition vector and add it to data frame (5)
intake_condition_new <- aac_intakes_outcomes$intake_condition
aac_intakes_outcomes <- cbind(aac_intakes_outcomes, intake_condition_new)
#define the new intake condition to be care if it was nursing or pregnant (5)
aac_intakes_outcomes$intake_condition_new[aac_intakes_outcomes$intake_condition_new == "Nursing" | aac_intakes_outcomes$intake_condition_new == "Pregnant"] <- "Care"
#define new intake condition to be other if not one of the desired conditions (5)
aac_intakes_outcomes$intake_condition_new[aac_intakes_outcomes$intake_condition_new != "Care" & aac_intakes_outcomes$intake_condition_new != "Normal" & aac_intakes_outcomes$intake_condition_new != "Sick" & aac_intakes_outcomes$intake_condition_new != "Injured"] <- "Other"


 
```

#### STEP 2

Run a linear regression that models `time_in_shelter_days` as a function of `intake_type`, `intake_condition_new` (created in Step 1), and `age_upon_intake_years`. 

```{r}
# Your code here!

#create linear regression model
model <- lm(formula = time_in_shelter_days ~ intake_type + intake_condition_new + 
    age_upon_intake_years, data = aac_intakes_outcomes)
#show summery of model
summary(model)

#calculate R-squared by hand
pred_val <- predict(model, newdata = aac_intakes_outcomes)
rss <- sum((aac_intakes_outcomes$time_in_shelter_days - pred_val)**2)
tss <- sum((aac_intakes_outcomes$time_in_shelter_days - mean(pred_val))**2)
rsqrt <- 1- (rss/tss)
rsqrt
```

We create the linear regression model ad defined in the task using the lm function and view a summary of that model.

#### STEP 3

Report coefficients and R-squared. Which coefficients are statistically significant? Interpret what they mean in terms of the feature they are associated with and the outcome.

The coefficients are:
                            Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  31.18137    1.84163  16.931  < 2e-16 ***
intake_typePublic Assist     -4.99086    0.83916  -5.947 2.74e-09 ***
intake_typeStray             -5.52506    0.53230 -10.380  < 2e-16 ***
intake_condition_newInjured  -3.37302    2.08328  -1.619 0.105436    
intake_condition_newNormal  -12.78785    1.80641  -7.079 1.47e-12 ***
intake_condition_newOther   -10.15392    3.04752  -3.332 0.000863 ***
intake_condition_newSick    -14.73927    2.29942  -6.410 1.47e-10 ***
age_upon_intake_years         0.53189    0.07152   7.437 1.05e-13 ***

R-squared:  0.00631

We can see that all coefficients besides intake condition new injured are highly significant. 
To interpret the coefficients, we have to separate between quantitative and qualitative variables. For quantitative variables like age upon intake year, hte coefficient means that an increase of 1 in it leads on an increase of 1 in y while all other variables are help constant. For example, an additional year of age upon intake leads to around 0.5 more days in the length of stay while all other variables are held constant. 
For qualitative variables, we use dummy variables. For example, intake type can either be owner surrender, public assist, or stray. We have two dummy variables: one for public assist and one for stray. If the intake type is owner surrenders, both are zero, and the model does not change. If the intake type is public assist, the public assist variable is 1 and the stray variable 0. Therefore, we add the coefficient for public assist into the model. This leads to around 5 less days compared to owner surrender if all other variables do not change. 
We can see that R squared is very low, which means that only a small amount of the data fit the regression model.

Then calculate R-squared by hand (as in, computing the quantities that make up R-squared rather than using the built-in one) and show that you get the same or nearly the same answer as the summary `lm` command.

Write out hand calculations here.

R-squared is defined as one minus the the sum of squares of residuals (RSS) over the sum total sum of squares(TSS). We get the formula (in R):
rsqrt <- 1- (rss/tss)

The RSS is defined as the sum of the all square roots of the actual values minus its prediction from our model. In R we get:
rss <- sum((aac_intakes_outcomes$time_in_shelter_days - pred_val)**2)
To use this formula, we need to calculate the predictions. We can use the predict function in R:
pred_val <- predict(model, newdata = aac_intakes_outcomes)

The TSS is defined as the sum of the sqare roots of the actual values minus the mean of all predictions. In R we get:
tss <- sum((aac_intakes_outcomes$time_in_shelter_days - mean(pred_val))**2)

Putting everything together and in the right order, we can calculate the RSS in R with:
#calculate R-squared by hand
pred_val <- predict(model, newdata = aac_intakes_outcomes)
rss <- sum((aac_intakes_outcomes$time_in_shelter_days - pred_val)**2)
tss <- sum((aac_intakes_outcomes$time_in_shelter_days - mean(pred_val))**2)
rsqrt <- 1- (rss/tss)

We can see that our result is similar to the one obtained from the model summary (about 0.006). 

#### STEP 4

The shelter is specifically interested in the effect of age upon intake on the length of stay. Set all predictors at the following values: `intake type` = stray and `intake_condition` = normal. Use them to create a data visualization that shows the 95% confidence interval of the expected values of `time_in_shelter_days` as `age_upon_intake_years` varies between the following values: 0.1, 0.5, 1, 2, 3, 5, 7, 10, 12, 15, 18, 20.  

Follow this procedure using a simulation-based approach:

1. Generate 1,000 sets of coefficients from your regression model.
2. Generate 1,000 predictions with each value of `age`.
3. Obtain 95% confidence intervals from these predictions. 
4. Plot your results.

Be sure to include axes labels and figure titles.

```{r}
# Your code here!

#generate 1000 sets of coefficients from regression model (1)
sim_model <- sim(model, n.sim = 1000)

#define vector of age values 
age_val <- c(0.1, 0.5, 1, 2, 3, 5, 7, 10, 12, 15, 18, 20)
#define vector for column names of different age predictions
age_c_names <- as.character(c(0, 0.1, 0.5, 1, 2, 3, 5, 7, 10, 12, 15, 18, 20))
#create indexes for age predictions
ind <- c(1:1000)
#create dataframe with indexes for age predictions
age_pred <- cbind(ind)
#for loop that creates predictions for all age values and adds them to data frame
for(i in 1:12) {
   current <- sim_model@coef[,1] + sim_model@coef[,3] + sim_model@coef[,5] + age_val[i] * sim_model@coef[,8]
  age_pred <- cbind(age_pred,current)
}
#rename columns
colnames(age_pred) <- age_c_names
#print head of data frame (2)
head(age_pred)

#create vectors for means and upper/lower bound of confidence interval
age_means <- c()
age_conf_low <- c()
age_conf_high <- c()

#for loop that adds means and bounds of 95% confidence interval to vectors
for(i in 2:13) {
  age_means <- append(age_means, mean(age_pred[,i]))
  age_conf_low <- append(age_conf_low, quantile(age_pred[,i], probs = c(0.025)))
  age_conf_high <- append(age_conf_high, quantile(age_pred[,i], probs = c(0.975)))
}
#unname bounds
unname(age_conf_low)
unname(age_conf_high)
#create data frame (3)
pdata <- data.frame(age_val, age_means, age_conf_low, age_conf_high)

#plot the data with errorbars (4)
ggplot(pdata, aes(age_val,)) + geom_point(
     data = pdata, 
     aes(age_val, age_means), 
     size = 2) + geom_errorbar(aes(ymin=age_conf_low, ymax=age_conf_high, width=.3))+labs(title="effect of ago upon intake on the length of stay of strayed dogs under normal intake condition" , x ="age of dogs in years", y = "time in shelter in days")
  
  
```


#### STEP 5

Write a short paragraph with your reflections on this exercise: 

1. What are the top 1-2 insights that the shelter can learn from your regression results and data visualization?
2. How does the prediction for length of stay changes with age, and how confident is that prediction?

One of the most important insight the our analysis is, that there is a positive relationship between the age upon intake and the time in shelter. This means that the older a dog is at intake, the longer the dog will be in the shelter. The coefficient is around 0.5, meaning for each additional year of age the dog stays 0.5 days longer if nothing else changes. The graph confirms a positive linear relationship for the specific case that the intake condition is normal and the intake type is stray. 
For the confidence intervals, the lower and the upper bounds are increasing as age increases, so we can be fairly certain about a positive relationship between days in shelter and age. The confidence intervals get larger as age increases, so we can make predictions with less certainty for older dogs.
Another important insight is that nearly all variables we included in the original model are significant. This shows, that we should include most of them to make good predictions. 

## QUESTION 4: Different regressions in an RCT on the same set of data

1. Answer the questions below after running [this code](tinyurl.com/yt4ml5ht)

2. Which regression specification(s) correctly identifies the data generating process?

The regression specification lm2 <- lm(ed2 ~ treat + ed1) correctly identifies the data generation process. ed2 is the dependent variable and it depends on ed1 and the treatment.
Further, the specification lm3 <- lm(ed3 ~ treat) correctly identifies the DGP. ed3 is ed2-ed1. If the treat is 0, ed2 and ed1 are the same and ed3 is 0. Otherwise ed2-ed1 is the treatment effect and ed3 is 5. Therefore, ed3 does solely depend on treat, as specified in the model. 

3. Which regression specification(s) reliably estimate(s) the treatment effect?

The average treatment affect is 5 The regression specifications:
lm1 <- lm(ed2 ~ treat)
lm2 <- lm(ed2 ~ treat + ed1)
lm3 <- lm(ed3 ~ treat)
lm5 <- lm(ed2 ~ treat + ed1 + I(treat*ed1)

estimate the treatment effect to be around 5.

4. What does the coefficient in the interaction term in lm5 imply? Is that implication accurate? 

It implies that there is an interaction term between ed1 and the treatment. This means that a change in one does automatically change the other as well. This is not true, because there is no such interaction term in the DGP. If we apply treatment or not does not change ed1 and the other way around. The coefficient is negative, so it suggests, that a increase of one in treat leads to a small decrease in ed1. The coefficient is not significant, and so close to zero, that we can see that we should not include it in a model. 

5. Why does lm1 deliver the intercept that it does?

It does, because it does not have any information about ed1. Therefore, the intercept represents ed2 if treat is 0. From the DGP, we know that is this case ed2 is equal to ed1. The mean of ed1 is ca 14, and so is the intercept, because this is the best prediction we can make for this model if we have no treatment.

6. Which is your favorite regression specification, and why, given whatever it is that you are trying to learn from the regression (which is up to you!)?  Which regression specification do you deem the worst, and why?

I think that my favorite model is:
lm3 <- lm(ed3 ~ treat)
This is, because it allows us to evaluate the treatment effect directly without considering ed2 or ed1. This can be useful, if we are interested in the treatment effect of a more complicated study.
I think taht lm4 is the worst model, because it is the only one that fails to predict a treatment effect close to the actual treatment effect. 


## QUESTION 5: A Classification problem

Beyond the time spent in the shelter, another important prediction question for the shelter is the outcome of the animal. Shelters typically measure themselves according to their ‘live release rate’: the fraction of animals with a live outcome out of all animals that arrived at the shelter. In this section, you’ll use similar features to predict a dog’s outcome, rather than the time spent at the shelter.

For that purpose, you’ll have to create a new binary feature called `live_release`. We shall define live release as having all but the following `outcome_type` values: Died, Disposal, Euthanasia, Missing.

You should maintain the same exclusions as the previous sections, since euthansia requests may (although this varies between shelters) not count towards the live release rate. If you’re wondering why, it’s because some euthanasia requests don’t actually have to end with euthanasia. Check how many of the 180 euthansia requests for dogs in this dataset were turned around. 


#### STEP 1

Create a `live_release` column based on the specification above. Then, choose any other feature in the dataset that you think might be interesting to include in the model, i.e., that you want to see whether it has any association with a dog’s live release chances. This may definitely include some additional feature engineering on your behalf (which can be as simple as choosing something like ‘black_color’ using the color feature). List your features here.



```{r}

#create new data frame for q5 (copy old one)
aac_intakes_outcomesq5 <- aac_intakes_outcomes
#create new vector live release as a copt of outcome type and add it to new df
live_release <- aac_intakes_outcomesq5$outcome_type
aac_intakes_outcomesq5 <- cbind(aac_intakes_outcomesq5, live_release)

#set new live release variable to true if appropriate
aac_intakes_outcomesq5$live_release[aac_intakes_outcomesq5$live_release!= "Died" & aac_intakes_outcomesq5$live_release != "Disposal" & aac_intakes_outcomesq5$live_release != "Euthanasia" & aac_intakes_outcomesq5$live_release != "Missing"] <- TRUE
#set live release to false otherwise
aac_intakes_outcomesq5$live_release[aac_intakes_outcomesq5$live_release!= TRUE] <- FALSE

#create new fertile vector based on outcome and add it to df
fertile <- aac_intakes_outcomesq5$sex_upon_outcome
aac_intakes_outcomesq5 <- cbind(aac_intakes_outcomesq5, fertile)
#change fertile variable to true if appropriate
aac_intakes_outcomesq5$fertile[aac_intakes_outcomesq5$fertile!= "Spayed Female" & aac_intakes_outcomesq5$fertile != "Neutered Male"] <- TRUE
#set fertile variable to false otherwise
aac_intakes_outcomesq5$fertile[aac_intakes_outcomesq5$fertile!= TRUE] <- FALSE
#delete rows with NA in live release or fertile 
aac_intakes_outcomesq5 <- aac_intakes_outcomesq5[!(is.na(aac_intakes_outcomesq5$live_release)), ]
aac_intakes_outcomesq5 <- aac_intakes_outcomesq5[!(is.na(aac_intakes_outcomesq5$fertile)), ]
#change fertile and live release to resemble vectors
aac_intakes_outcomesq5$fertile <- factor(aac_intakes_outcomesq5$fertile)
aac_intakes_outcomesq5$live_release <- factor(aac_intakes_outcomesq5$live_release)
```
First, we create a new data frame for the question 5. We create the variable live release and set it to true or false based on the definition. For the additional feature, I decided to use fertility. I want to see if the fertility upon outcome might have an effect on the live realse. I decided to set fertile to true if the dog is fertile and to false otherwise. This means that the few missing data points are going to be false. I choose that because I want to know if fertility has an effect, and if the fertility is not known the fertility could not impact the decision. Therefore, fertility can be seen as known fertility. 

#### STEP 2

This time, we want to also be able to see how good our predictions are on unseen data. While there are several variations of the k-fold cross-validation method, let’s stick with the simplest one where we just split randomly the dataset into a training and a testing (aka validation) set.

Randomly select 80% of the data to be put in a training set and leave the rest for a test set.

```{r}
# Your code here!

#estimate sample size
smp_size <- floor(0.80 * nrow(aac_intakes_outcomesq5))
#create indexes for the training data
train_ind_q5 <- sample(seq_len(nrow(aac_intakes_outcomesq5)), size = smp_size)

#split data based on indexes
train_model <- aac_intakes_outcomesq5[train_ind_q5, ]
test_model <- aac_intakes_outcomesq5[-train_ind_q5, ]

```

We split the data in to roughly 80% training data and 20% test data.
#### STEP 3

Using your training set (only!), run a logistic regression, modeling `live_release` as a function of `intake_type`, `intake_condition`, and `age_upon_intake_years`, and your other features. Report and interpret the regression coefficient and 95% confidence intervals for `age_upon_intake`.

```{r}
# Your code here!

#create the model with training data
model_q5 <- glm(live_release~ intake_type + intake_condition_new + age_upon_intake_years + fertile, data=train_model, family="binomial")

#print summary of model
summary(model_q5)
#print confidence intervall of coeficients 
confint(model_q5)
```


We can see that all variables besides intake condition being normal or other were significant. Further, we can see that coefficients with a positive sign and make a live release more likely if true while coefficients with negative signs make a live release less likely. For example, if the dog is fertile, this will make its live release less likely, will a dog that is the type stray, will make a live release more likely. We can also compare coefficients between each other. For example, the coefficient of the intake type stray is approximate double the stray, witch means that, if all other variables stay the same, the effect of being a public assist is twice as big as the effect of being a stray. 
We can see, that most confidence are quite different. For example, the coefficients for fertility and intake type stray are fairy tight, so that we can say that we have meaningful coefficients. Other confidence interval reveal uncertainty. For example, the inrake type normal is different signs within its confidence interval, so we cannot be sure about the direction of its impact. 
#### STEP 4

Use the logistic regression model to predict the live release outcomes on the test set. Start by using 0.5 as a threshold, and show your confusion matrix.

```{r}
# Your code here!

#create probabilities on test set
pred_prob <- predict(model_q5, newdata = test_model, type="response")
#create predictions based on probabilities and threshold 0.5
pred_lr <- pred_prob > 0.5
#show confusion matrix
confusionMatrix(factor(pred_lr), factor(test_model$live_release))
```


#### STEP 5

Is it possible that another threshold will be better for this model? Try a few different ones and show one that results in better prediction performance (if none do, show one that gives a worse result). Justify your choice, explaining why the different error types made by changing the threshold is preferable to your previous result. 

```{r}
# Your code here!

#create vector for thresholds
threshs <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.7, 0.8, 0.9)
#for loop to create predictions with different thresholds and print accuracy
for (i in threshs) {
  cur_lr <- pred_prob > i
  cf <- confusionMatrix(factor(cur_lr), factor(test_model$live_release))
  print(i)
  print(cf$overall['Accuracy'])
  print("   ")
}

```
Based on the results I would choose a threshold of 0.3. This is because it gives the highest accuracy. We have to note thought that all accuracies are close together, and we could have other reasonable choices for a threshold. For example, we could consider whether false positives or false negatives are more costly when making wrong predictions. 

#### STEP 6: Bonus Question

Write code or use code from existing packages that you come across to create the ROC curve for this classification problem!

```{r}
# Your code here!

```

#### STEP 7

Lastly, write a short summary (1-2 paragraphs) with your reflections on the exercise, including: 

1. Reasoning for the choices you made in your feature selection.
2. What other variables might have been interesting to look at which are not available in the data.
3. How good are your model’s predictions on the test set? What might explain it (that predictions are or are not highly accurate) and how could it be improved?

I chose the fertility, because I thought that this might have an effect on the live release. I saw that less dogs were fertile at the outcome than at the income, so I thought that this intervention might lead to live release. The fertility variable I created had significant impact on the live release probability, and quite big negative coefficient. This showed that fertility, makes it less likely to be have a live release. There are various other variables like color or founding location that might have an impact on live release. The predictions are quite accurate overall, but this is mostly due to the fact that live release is so often true. Whenever the model predicts false, the error margin is very high. This is quite impractical, becuase it makes our model not much better than a take the best approach of predicting the common outcome (true). This is, because we just have limited data available, and have additionally, have only used parts of that data and parts of methods to combine and model the data. With more time, we might find a set of variables that leads to better predictions. 


# End of Assignment

## Final Steps

Before finalizing your project you'll want to be sure there are **comments in your code chunks** and **text outside of your code chunks** to explain what you're doing in each code chunk. These explanations are incredibly helpful for someone who doesn't code or someone unfamiliar to your project.

You have two options for submission:

1. You can complete this .rmd file, knit it to pdf and submit the resulting .pdf file on Forum.
2. You can complete the Google Doc version of this assignment, include your code, graphs, results, and your explanations wherever necessary and download the Google Doc as a pdf file and submit the pdf file on Forum. If you choose this method, you need to make sure you will provide a link to an .R script file where your code can be found (you can host your code on Github or Google Drive). Note that links to Google Docs are not accepted as your final submission.


### Knitting your R Markdown Document

Last but not least, you'll want to **Knit your .Rmd document into a pdf document**. If you get an error, take a look at what the error says and edit your .Rmd document. Then, try to Knit again! Troubleshooting these error messages will teach you a lot about coding in R. If you get any error that doesn't make sense to you, post it on Piazza.


Good Luck! The Teaching Team